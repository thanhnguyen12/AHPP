\documentclass[12pt]{article}
\usepackage{amsmath, bm, graphicx}
\usepackage[margin=0.75in]{geometry}
\usepackage[parfill]{parskip}
\usepackage[none]{hyphenat}

\begin{document}
\noindent
Ben Buzzee, Thanh Nguyen, Kellie McClernon \\
Team name: NotTooDeep\_Learning \\
\today
\vspace{0.5 cm}

\textbf{Current placement}: 673 (out of 2222)

\textbf{Kaggle Placement}
\begin{center}
\begin{tabular}{l c c c c}
\hline
Week & Model & CV Error & Test Error & Placement \\
\hline
April 3 & GBM & 0.1307 & 0.13066 & 1161 \\
April 10 & Ensemble & 0.116 & 0.12168 & 673 \\
\hline
\end{tabular}
\end{center}

\textbf{Best Fitted Models}
\begin{center}
\begin{tabular}{l c c}
	\hline
	Model & CV Error & Test Error \\
	\hline
	Ensemble & 0.116 & 0.12168 \\
	PLS & 0.1255 & \\
	GBM & 0.1307 & 0.13066 \\
	PCR & 0.1412 & \\
	Ridge & 0.1437 & \\
	\hline
\end{tabular}
\end{center}

\textbf{April 10th}

Our goal for this week was dimension reduction.  To accomplish this, we performed a PCA to see if we could identify any predictors with high eigenvalues or very low values.  However, most values were very high and fairly close to each other.  We suspect this is because of the high correlation among our variables.  A possible next step is to look into a cluster analysis of our predictors since clustering highly correlated variables could help us to reduce the number of predictors.  % Additionally, we know that methods such as random forest overly favor highly correlated variables and thus are prone to choosing variables that are not directly related to the response but rather correlated with another predictor that is.

We also tried Lasso as a method of predictor selection by looking for near-zero coefficients.  Originally the data set contains 80 predictors, many of which are related.  We are convinced that not all predictors are necessary and if we reduce the ``noisy" predictors our method of choice will be better able to select those that truly influence the response.  Additionally, we would like to consider interactions, but interactions on 80 variables quickly explodes and our dataset is not very deep.

We had considered linear models but given the large number of categorical predictors we think that tree based models might have the most potential.  We are noticing that so far, without much feature engineering, all our models have roughly the same CV, around 0.13.  We need to find a way to improve our feature selection in order to get at least one standalone model with error rate closer to 0.10.  We know we want to ensemble our models but if all the models we ensemble have an similar error rate, the ensembling does not give us a very significant lift.  In fact, our first ensemble was with three models, all of which had similar error rates of 0.14 to 0.13, and combining them got us a CV error rate of about 0.12, which is not a very large improvement.  We are currently considering group lasso as a way to deal with natural groupings of predictors in the data set.

For next week, we are planning to explore the predictors with near zero variance, i.e. columns that have essentially the same category for all fields in our training, and converting the sale month to season.  Neighborhood has a lot of categories and thus is being over-emphasized in tree models.  We would like the reduce the number of categories for this predictor; we are considering doing this by mean sale price.

\newpage
\textbf{April 3rd}

This week our team focused on exploring and cleaning the Ames Housing data. We found that most of the NAs present had known values mentioned in the data description document provided by Kaggle. After replacing the NAs with known values, we still had a handful of missing values to deal with. To allow us to fit models we replaced those missing values with medians for quantitative variables or modes for categorical. More work will be needed to ensure replaced missing values are well chosen.

After fitting a handful of individual models, we decided to submit our best individual model to provide us with a baseline RMSE we can strive to improve. The generalized boosted regression model ('gbm' in caret) provided the best repeated cross-validation RMSE at .1307 (for log transformed saleprice). Our submission results were surprisingly similar with a RMSE of .13066. Our current rank is 1161 out of 2245 teams. Now that we have a "clean" dataset, we can now focus on dimension reduction, feature engineering, and combining models.

\end{document}